# -*- coding: utf-8 -*-
"""Mech Design for Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jLbchsIaYv4antdbg2smDN9J_NGRotiv
"""

from google.colab import drive
drive.mount('/content/gdrive')

import numpy as np
from pandas import read_csv
from sklearn.model_selection import train_test_split
import torch
import math
import random
_percentTrainData = 0.8

dataset = read_csv("/content/gdrive/MyDrive/Breast_Cancer_data.csv",header = 0)            # loads the dataset. link for the dataset is provided in the references
dataset = dataset.drop('id', 1)
dataset = dataset.drop('Unnamed: 32', 1)
data = dataset.values
X = data[:,1:31].astype(float)
Y = data[:,0]
Y[ Y == 'M'] = 1
Y[ Y == 'B'] = -1
Y = Y.astype(float)
validation_size = 0.20
seed = 7
X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
    test_size=validation_size, random_state=seed)

X_train = (X_train - X_train.mean(axis=1, keepdims=True))/(np.amax(X_train, axis=1, keepdims=True)-np.amin(X_train, axis=1, keepdims=True))/math.sqrt(31)
X_test = (X_test - X_test.mean(axis=1, keepdims=True))/(np.amax(X_test, axis=1, keepdims=True)-np.amin(X_test, axis=1, keepdims=True))/math.sqrt(31)

print(np.linalg.norm(X_train[1]))

Y_train = np.reshape(Y_train,(Y_train.size,1))
Y_test = np.reshape(Y_test,(Y_test.size,1))

train_size = np.shape(X_train)
test_size = np.shape(X_test)
print(train_size)
print(test_size)

X_train = torch.from_numpy(X_train)
Y_train = torch.from_numpy(Y_train)
X_test = torch.from_numpy(X_test)
Y_test = torch.from_numpy(Y_test)

X_train = X_train.float()
Y_train = Y_train.float()
X_test = X_test.float()
Y_test = Y_test.float()

m = train_size[0]
n = train_size[1]

a = 0.0001                                                                                      #### here we initialise the hyperparameter space
b = 0.0005
c = a + (b-a)*torch.rand(m,1)
psi = 2.0*c - a
lr = 1
#lambd0_mat = [0.00001]
lambd0_mat = [0.0001,0.00001,0.000001]
#lambd1_mat = [300]
lambd1_mat = [3, 30, 300]
gamma_mat = [0.01, 0.1, 1, 10, 100]
#lambd2_mat = [10000]
lambd2_mat = [1000,10000]

b_unit = torch.rand(n,1)                                                              ##### sample noise
b_unit = b_unit/torch.norm(b_unit)
dist = torch.distributions.gamma.Gamma(n, torch.tensor([1.0]))
noise = dist.sample()
print(noise)

import cvxpy as cp

for gamma in gamma_mat:
  flag = 0
  min_loss = 100000
  min_emp_loss = 100000
  min_gen_loss = 100000
  min_priv_loss = 100000
  min_train_acc = 0
  min_test_acc = 0
  lambd0_opt = 0
  lambd1_opt = 0
  e_opt = 0

  for lambd0 in lambd0_mat:
    for lambd1 in lambd1_mat:
      for lambd2 in lambd2_mat:
        for e_avg in range(200,290,100):
          print("lambd0: ", lambd0, " lambd1: ", lambd1, " lambd2: ", lambd2, " e_avg: ", e_avg)
          flag = 1
          a = torch.ones(m,1)
          a_grad = torch.rand(m,1)
          w_opt = torch.rand(n,1)
          w_grad = torch.randn(n,1)
          a = torch.log(a/m)
          a_old = torch.rand(m,1)
          a.requires_grad_(True)
          w_opt.requires_grad_(True)
          max_reg = 0

          for i in range(300000):
            if(torch.norm(a - a_old) < 1e-4 and torch.norm(w_grad)/torch.norm(w_opt) < 1e-5):
              break

            emp_loss = torch.sum(torch.mul(torch.exp(a),torch.log2(1+torch.exp(-torch.mul(Y_train,torch.matmul(X_train,w_opt)))))) + torch.matmul(torch.transpose(w_opt,0,1),b_unit)*noise*2.0/(m*e_avg*0.01)
            gen_loss = lambd1*torch.norm(torch.exp(a)) + lambd2/(m*e_avg) + lambd0/2*torch.norm(w_opt)**2
            priv_loss = (1.0*gamma*m)*e_avg*0.01*torch.sum(torch.mul(torch.exp(a),psi))
            priv = m*e_avg*0.01*torch.sum(torch.mul(torch.exp(a),psi))
            loss = (emp_loss + gen_loss) + priv_loss                                              #### calculating the loss
            loss.backward()

            y_opt = torch.reciprocal(1.0+torch.exp(-torch.matmul(X_train,w_opt)))
            y_opt[y_opt<0.5] = -1
            y_opt[y_opt>0.5] = 1
            diff = torch.abs(Y_train-y_opt)/2
            with torch.no_grad():                                                                         ### projected gradient descent
              a_old = a.detach().clone()
              w_grad = w_opt.grad.clone().detach()
              updated_a = a - a.grad*lr
              ref = (torch.exp(updated_a)).numpy()
              ref = np.squeeze(ref)
              new_a = cp.Variable(m)
              objective = cp.Minimize(cp.norm(new_a-ref)**2)
              #constraints = [cp.sum(new_a) == 1, new_a >= 1e-8, new_a <= 1, new_a <= np.squeeze(epsilon)/(m*0.01*e_avg)]         ### use the constraint set depending on the problem
              constraints = [cp.sum(new_a) == 1, new_a >= 1e-8, new_a <= 1]
              prob = cp.Problem(objective, constraints)
              result = prob.solve()
              mid = torch.from_numpy(new_a.value)
              mid = torch.reshape(mid,(m,1))
              updated_a = torch.log(mid)
              updated_a = updated_a.detach()
              a.copy_(updated_a)
              updated_w = w_opt - w_opt.grad*lr
              updated_w = updated_w.detach()
              w_opt.copy_(updated_w)
              a.grad.zero_()
              w_opt.grad.zero_()

          y_opt_test = torch.reciprocal(1.0+torch.exp(-torch.matmul(X_test,w_opt)))
          y_opt_test[y_opt_test<0.5] = -1
          y_opt_test[y_opt_test>0.5] = 1
          diff_test = torch.abs(Y_test-y_opt_test)/2
          test_loss = torch.count_nonzero(diff_test)/test_size[0] + priv_loss
          y_opt = torch.reciprocal(1.0+torch.exp(-torch.matmul(X_train,w_opt)))
          y_opt[y_opt<0.5] = -1
          y_opt[y_opt>0.5] = 1
          diff = torch.abs(Y_train-y_opt)/2
          print("emp_loss: ",emp_loss)                                                                      #### choose the best combination of hyperparameters based on requirement
          print("gamma: ", gamma)
          print("loss: ", min_loss)
          print("priv_loss: ", priv)
          print("train_accuracy: ", 1-torch.count_nonzero(diff)/m)
          print("test_accuracy: ", 1-torch.count_nonzero(diff_test)/test_size[0])
          print("e_avg: ", e_avg)
          print("lambd0: ", lambd0)
          print("lambd1: ", lambd1)
          print("lambd2: ", lambd2)
          print("std_dev: ",torch.std(torch.exp(a)))